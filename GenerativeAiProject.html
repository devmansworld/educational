<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>LLM Python Project & Secure Automation Reference</title>
<style>
  body { font-family: "Segoe UI", sans-serif; background: #f4f4f9; color:#222; margin:40px; }
  h1, h2 { color:#ff6600; }
  code, pre { background:#222; color:#eee; padding:10px; border-radius:6px; display:block; overflow:auto; }
  .note { background:#ffffcc; padding:10px; border-left:4px solid orange; }
</style>
</head>
<body>
<h1>ğŸ”’ Generative AI / LLMs â€“ Python Automation Teaching Reference</h1>

<p>This document summarizes the secure structure and logic of the practical project 
used to demonstrate <strong>LLM-based automation and evaluation</strong>.  
It includes folder organization, safe pseudocode, and a guide for reproducibility.</p>

<hr>
<h2>ğŸ“ Folder Structure</h2>
<pre>
llm_edu_project/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ prompts.json
â”‚   â”œâ”€â”€ results/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ llm_engine.py
â”‚   â”œâ”€â”€ evaluator.py
â”‚   â””â”€â”€ tests/
â”‚       â””â”€â”€ test_llm_responses.py
â”‚
â””â”€â”€ run.sh
</pre>

<hr>
<h2>âš™ï¸ Sample Python â€“ llm_engine.py</h2>
<pre>
# llm_engine.py
import openai
import json

def generate_completion(prompt: str, temperature=0.7, max_tokens=150):
    """
    Generates LLM completion based on user prompt.
    Safe version without API keys stored in code.
    """
    # Instructors should inject credentials via environment variables
    completion = {
        "prompt": prompt,
        "temperature": temperature,
        "max_tokens": max_tokens
    }
    print("ğŸ§  Simulating LLM output:", json.dumps(completion, indent=2))
    return completion
</pre>

<hr>
<h2>ğŸ§ª Automated Evaluation â€“ evaluator.py</h2>
<pre>
# evaluator.py
from src.llm_engine import generate_completion

def evaluate_model(prompt_list):
    results = []
    for prompt in prompt_list:
        result = generate_completion(prompt)
        results.append(result)
    return results
</pre>

<hr>
<h2>ğŸ§  Secure Test Example â€“ test_llm_responses.py</h2>
<pre>
# test_llm_responses.py
from src.evaluator import evaluate_model

def test_responses_structure():
    prompts = ["Translate 'bonjour' to English", "Summarize AI ethics"]
    results = evaluate_model(prompts)
    assert all("prompt" in r for r in results)
    print("âœ… Secure evaluation test passed.")
</pre>

<hr>
<h2>ğŸ§° Execution Guide</h2>
<ul>
  <li>Run tests safely in local environment (no external API keys).</li>
  <li>Validate prompt-response logic and store logs under <code>data/results/</code>.</li>
  <li>In a teaching setup: distribute only this HTML + safe scripts, keep credentials internal.</li>
</ul>

<div class="note">
ğŸ’¡ Tip: For secure deployment in a classroom, keep the <code>llm_engine.py</code> mock-based.
Real LLM calls can be enabled only by authorized instructors using environment-provided tokens.
</div>

<hr>
<footer><em>Teaching Reference Â© 2025 â€” Generated for private academic use only.</em></footer>
</body>
</html>
